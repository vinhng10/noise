# lightning.pytorch==2.1.3
fit:
  seed_everything: 300495
  trainer:
    accelerator: auto
    strategy: auto
    devices: auto
    num_nodes: 1
    precision: null
    logger:
      class_path: lightning.pytorch.loggers.TensorBoardLogger
      init_args:
        save_dir: ./logs
        name: null
        version: kd-1.0.2
        log_graph: false
        default_hp_metric: false
        prefix: ""
        sub_dir: null
    callbacks:
      - class_path: lightning.pytorch.callbacks.ModelCheckpoint
        init_args:
          dirpath: null
          filename: "{epoch}-{train_loss:.3f}"
          monitor: train_loss
          verbose: false
          save_last: true
          save_top_k: 10
          save_weights_only: false
          mode: min
          auto_insert_metric_name: true
          every_n_train_steps: null
          train_time_interval: null
          every_n_epochs: 1
          save_on_train_epoch_end: null
          enable_version_counter: true
      - class_path: lightning.pytorch.callbacks.LearningRateMonitor
        init_args:
          logging_interval: "epoch"
          log_momentum: false
    fast_dev_run: false
    max_epochs: -1
    min_epochs: null
    max_steps: -1
    min_steps: null
    max_time: null
    limit_train_batches: null
    limit_val_batches: null
    limit_test_batches: null
    limit_predict_batches: null
    overfit_batches: 0.0
    val_check_interval: null
    check_val_every_n_epoch: 1
    num_sanity_val_steps: null
    log_every_n_steps: null
    enable_checkpointing: null
    enable_progress_bar: null
    enable_model_summary: null
    accumulate_grad_batches: 1
    gradient_clip_val: null
    gradient_clip_algorithm: null
    deterministic: null
    benchmark: null
    inference_mode: true
    use_distributed_sampler: true
    profiler: null
    detect_anomaly: false
    barebones: false
    plugins: null
    sync_batchnorm: false
    reload_dataloaders_every_n_epochs: 0
    default_root_dir: null
  model:
    student:
      in_channels: 1
      hidden_channels: 16
      max_channels: 64
      out_channels: 1
      encoder_n_layers: 4
      nhead: 2
      num_layers: 2
      dropout: 0.0
      bias: true
    teacher:
      channels_input: 1
      channels_output: 1
      channels_H: 64
      max_H: 768
      encoder_n_layers: 8
      kernel_size: 4
      stride: 2
      tsfm_n_layers: 5
      tsfm_n_head: 8
      tsfm_d_model: 512
      tsfm_d_inner: 2048
      ckpt_path: ./data-debug/checkpoints/pretrained.pkl
    mr_stft_lambda: 1e-1
    fft_sizes: [512, 1024, 2048]
    hop_lengths: [50, 120, 240]
    win_lengths: [240, 600, 1200]
  data:
    data_dir: ./data-debug
    sampling_rate: 16000
    length: 4096
    num_samples: 10000
    num_workers: 2
    batch_size: 16
  optimizer:
    lr: 1.0
    betas:
      - 0.9
      - 0.999
    eps: 1.0e-08
    weight_decay: 0
    amsgrad: false
    maximize: false
    foreach: null
    capturable: false
    differentiable: false
    fused: null
  lr_scheduler:
    max_lr: 2e-4
    min_lr: 2e-4
    warmup_steps: 100
    training_steps: 100
    num_cycles: 0.5
  # ckpt_path: logs/mobilenetv1-1.0.4/checkpoints/last.ckpt
